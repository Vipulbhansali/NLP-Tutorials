{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d378939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Vipul\\IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b929b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d085e9a",
   "metadata": {},
   "source": [
    "# Do Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861ab261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deccd81",
   "metadata": {},
   "source": [
    "# REMOVE HTML TAGS & URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbce32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    return clean_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff5c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da35ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Regex pattern to match URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Replace URLs with an empty string\n",
    "    clean_text = url_pattern.sub('', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2c069",
   "metadata": {},
   "source": [
    "# REMOVING PUNCTUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af2eebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "print(punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bdb8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \n",
    "    for char in punctuation:\n",
    "        text = text.replace(char,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6fe927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81649898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production The filming tech...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically theres a family where a little boy J...\n",
       "4        Petter Matteis Love in the Time of Money is a ...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot bad dialogue bad acting idiotic direc...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    Im going to have to disagree with the previous...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62213842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate method for big data set to remove punctuation\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation_2(text):\n",
    "    # Generate translation table mapping all punctuation characters to None\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "    # Remove punctuation using translate() method\n",
    "    clean_text = text.translate(translation_table)\n",
    "    return clean_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc0145",
   "metadata": {},
   "source": [
    "# CHAT WORD TREATMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f87bc364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughing out loud you are great! be right back talk to you later\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def chat_word_treatment(chat_message):\n",
    "    # Define a dictionary for abbreviation expansion\n",
    "    abbreviations = {\n",
    "        'lol': 'laughing out loud',\n",
    "        'brb': 'be right back',\n",
    "        'ttyl': 'talk to you later'\n",
    "        # Add more abbreviations as needed\n",
    "    }\n",
    "    \n",
    "    # Define a dictionary for slang conversion\n",
    "    slang = {\n",
    "        'u': 'you',\n",
    "        'r': 'are',\n",
    "        'gr8': 'great',\n",
    "        # Add more slang terms as needed\n",
    "    }\n",
    "    \n",
    "    # Normalize chat message to lowercase\n",
    "    chat_message_lower = chat_message.lower()\n",
    "    \n",
    "    # Replace abbreviations while preserving capitalization\n",
    "    for abbreviation, full_form in abbreviations.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(abbreviation) + r'\\b', re.IGNORECASE)\n",
    "        chat_message_lower = pattern.sub(full_form, chat_message_lower)\n",
    "    \n",
    "    # Replace slang terms while preserving capitalization\n",
    "    for term, replacement in slang.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(term) + r'\\b', re.IGNORECASE)\n",
    "        chat_message_lower = pattern.sub(replacement, chat_message_lower)\n",
    "    \n",
    "    return chat_message_lower\n",
    "\n",
    "# Example usage:\n",
    "chat_message = \"LOL u r gr8! brb ttyl\"\n",
    "processed_chat_message = chat_word_treatment(chat_message)\n",
    "print(processed_chat_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e063ba",
   "metadata": {},
   "source": [
    "# SPELLING CORRECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5fbd6ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spellchecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspellchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspell_correction_pyspellchecker\u001b[39m(text):\n\u001b[0;32m      4\u001b[0m     spell \u001b[38;5;241m=\u001b[39m SpellChecker()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spellchecker'"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def spell_correction_pyspellchecker(text):\n",
    "    spell = SpellChecker()\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    corrected_text = []\n",
    "    for word in words:\n",
    "        # Get the corrected version of each word\n",
    "        corrected_word = spell.correction(word)\n",
    "        corrected_text.append(corrected_word)\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "# Example usage:\n",
    "text = \"Thes is an example of spell correction using pyspellchecker.\"\n",
    "corrected_text_pyspellchecker = spell_correction_pyspellchecker(text)\n",
    "print(\"Pyspellchecker Corrected Text:\", corrected_text_pyspellchecker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def spell_correction_textblob(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = blob.correct()\n",
    "    return str(corrected_text)\n",
    "\n",
    "# Example usage:\n",
    "text = \"Thes is an example of spell correction using TextBlob.\"\n",
    "corrected_text_textblob = spell_correction_textblob(text)\n",
    "print(\"TextBlob Corrected Text:\", corrected_text_textblob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769af675",
   "metadata": {},
   "source": [
    "#  Removing Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bc41feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca2ab394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b114e862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00b8db71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vipul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55670570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Text: example sentence demonstrating removal stopwords using NLTK .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Get the list of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords from the list of words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a single string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is an example sentence demonstrating the removal of stopwords using NLTK.\"\n",
    "filtered_text = remove_stopwords(text)\n",
    "print(\"Filtered Text:\", filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c8e33bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vipul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8830e298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Text: Hello!  This is a message with emojis! \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # Define regex pattern to match emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    # Remove emojis from the text\n",
    "    clean_text = emoji_pattern.sub(r'', text)\n",
    "    return clean_text\n",
    "\n",
    "# Example usage:\n",
    "text_with_emojis = \"Hello! ðŸ˜Š This is a message with emojis! ðŸŒŸðŸš€\"\n",
    "clean_text = remove_emojis(text_with_emojis)\n",
    "print(\"Clean Text:\", clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2187ac",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fa573",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, phrases, or other meaningful elements depending on the context and the specific requirements of the task at hand. Tokenization is a fundamental preprocessing step in natural language processing (NLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07333371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Word Tokenization:\n",
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"Tokenization is the process of breaking down text into smaller units called tokens.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"NLTK Word Tokenization:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa18db",
   "metadata": {},
   "source": [
    "# Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c86be9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "spaCy Tokenization:\n",
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "# Download the 'en_core_web_sm' model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Load spaCy English tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Tokenization is the process of breaking down text into smaller units called tokens.\"\n",
    "\n",
    "# Tokenize the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tokens from spaCy document\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(\"spaCy Tokenization:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6b160",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e288d",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing words to their root or base form, typically by removing suffixes or prefixes. It's a common technique used in natural language processing (NLP) and information retrieval to normalize words and reduce them to a common form.\n",
    "\n",
    "There are various stemming algorithms available, with the Porter stemming algorithm being one of the most widely used. NLTK (Natural Language Toolkit) provides an implementation of the Porter stemming algorithm, which you can use in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5e723f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "I am loving this beautiful day. It's sunny outside and I'm going for a walk.\n",
      "\n",
      "Stemmed Text:\n",
      "i am love thi beauti day . it 's sunni outsid and i 'm go for a walk .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create a stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example text\n",
    "text = \"I am loving this beautiful day. It's sunny outside and I'm going for a walk.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Stem each word in the list of words\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Join the stemmed words back into a single string\n",
    "stemmed_text = ' '.join(stemmed_words)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nStemmed Text:\")\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf8fe1",
   "metadata": {},
   "source": [
    "\"this\" was stemmed to \"thi\", which is also a result of the rules applied by the stemming algorithm. While \"thi\" is not a valid English word, stemming algorithms prioritize the reduction of words to their base form, often resulting in such transformations.\n",
    "\n",
    "It's important to note that stemming is a crude heuristic process and may not always produce meaningful or correct results, especially for irregular words or words with complex morphology. In some cases, more advanced techniques like lemmatization, which considers the morphological analysis of words, may be preferred over stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147dc1af",
   "metadata": {},
   "source": [
    "# LEMMITIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95ed058e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vipul\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fed9e18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "I am loving this beautiful day. It's sunny outside and I'm going for a walk.\n",
      "\n",
      "Lemmatized Text:\n",
      "I be love this beautiful day . It 's sunny outside and I 'm go for a walk .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example text\n",
    "text = \"I am loving this beautiful day. It's sunny outside and I'm going for a walk.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Lemmatize each word in the list of words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word,pos='v') for word in words]\n",
    "\n",
    "# Join the lemmatized words back into a single string\n",
    "lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nLemmatized Text:\")\n",
    "print(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc91dd",
   "metadata": {},
   "source": [
    "Stemming is fast. Lemmitization is slow, if output has to be shown to user use Lemmitization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc98c3",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85003821",
   "metadata": {},
   "source": [
    "#  TEXT REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c354c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Matrix:\n",
      "[[0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 2 0 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1]\n",
      " [1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0]]\n",
      "Vocabulary:\n",
      "['an' 'brown' 'cheese' 'dog' 'eat' 'exciting' 'extra' 'field' 'fox' 'is'\n",
      " 'jumps' 'lazy' 'learning' 'love' 'machine' 'of' 'over' 'pizza' 'quick'\n",
      " 'study' 'the' 'to' 'with']\n",
      "\n",
      "Bigrams (2-grams) Matrix:\n",
      "[[0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1]\n",
      " [1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0]]\n",
      "Bigrams Vocabulary:\n",
      "['an exciting' 'brown fox' 'eat pizza' 'exciting field' 'extra cheese'\n",
      " 'field of' 'fox jumps' 'is an' 'jumps over' 'lazy dog' 'learning is'\n",
      " 'love to' 'machine learning' 'of study' 'over the' 'pizza with'\n",
      " 'quick brown' 'the lazy' 'the quick' 'to eat' 'with extra']\n"
     ]
    }
   ],
   "source": [
    " #Bag of words & N-grams\n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example text data\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"I love to eat pizza with extra cheese\",\n",
    "    \"Machine learning is an exciting field of study\"\n",
    "]\n",
    "\n",
    "# Bag of Words (BoW)\n",
    "def bow_representation(corpus):\n",
    "    # Create CountVectorizer object\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Fit and transform the corpus to create BoW representation\n",
    "    bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Get the vocabulary (unique words) from the CountVectorizer\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Convert BoW matrix to array and return with vocabulary\n",
    "    return bow_matrix.toarray(), vocabulary\n",
    "\n",
    "bow_matrix, vocabulary = bow_representation(corpus)\n",
    "print(\"Bag of Words (BoW) Matrix:\")\n",
    "print(bow_matrix)\n",
    "print(\"Vocabulary:\")\n",
    "print(vocabulary)\n",
    "print()\n",
    "\n",
    "# N-grams\n",
    "def ngram_representation(corpus, n):\n",
    "    # Create CountVectorizer object with specified n-gram range\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "\n",
    "    # Fit and transform the corpus to create n-gram representation\n",
    "    ngram_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Get the vocabulary (unique n-grams) from the CountVectorizer\n",
    "    ngram_vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Convert n-gram matrix to array and return with n-gram vocabulary\n",
    "    return ngram_matrix.toarray(), ngram_vocabulary\n",
    "\n",
    "# Example usage for bigrams (2-grams)\n",
    "ngram_matrix, ngram_vocabulary = ngram_representation(corpus, n=2)\n",
    "print(\"Bigrams (2-grams) Matrix:\")\n",
    "print(ngram_matrix)\n",
    "print(\"Bigrams Vocabulary:\")\n",
    "print(ngram_vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6bbc42",
   "metadata": {},
   "source": [
    "Drawbacks of Bag of Words (BoW):\n",
    "\n",
    "    Loss of word order and context\n",
    "\n",
    "    High dimensionality\n",
    "\n",
    "    Out-of-vocabulary words\n",
    "\n",
    "    Difficulty handling polysemy and synonymy\n",
    "\n",
    "    Fixed-length representation\n",
    "\n",
    "    Lack of semantic information\n",
    "\n",
    "\n",
    "Drawbacks of n-grams:\n",
    "\n",
    "    Increased Dimensionality\n",
    "    Data Sparsity\n",
    "    Lack of Generalization\n",
    "    Increased Memory Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242622c7",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used in natural language processing (NLP) and information retrieval to measure the importance of a term (word) in a document relative to a corpus of documents. It combines two metrics: term frequency (TF) and inverse document frequency (IDF).\n",
    "\n",
    "Term Frequency (TF): TF measures the frequency of a term (word) in a document. It is calculated as the ratio of the number of times a term appears in a document to the total number of terms in the document. TF gives higher weight to terms that appear more frequently in the document.\n",
    "\n",
    "Inverse Document Frequency (IDF): IDF measures the rarity of a term across the entire corpus of documents. It is calculated as the logarithm of the ratio of the total number of documents in the corpus to the number of documents containing the term. IDF gives higher weight to terms that are rare across the corpus, as they are more discriminative.\n",
    "\n",
    "TF-IDF Score: The TF-IDF score of a term in a document is the product of its TF and IDF scores. It indicates how important the term is in the document relative to the entire corpus. Terms with higher TF-IDF scores are considered more important and are often used as features in tasks such as text classification, information retrieval, and document similarity.\n",
    "\n",
    "In summary, TF-IDF assigns higher weights to terms that are frequent in the document but rare across the corpus, making it a useful metric for capturing the discriminative power of terms in text data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6407c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['brown' 'dog' 'fox' 'is' 'jumps' 'lazy' 'over' 'quick' 'the']\n",
      "TF-IDF Matrix:\n",
      " [[0.3744896  0.284809   0.284809   0.         0.3744896  0.3744896\n",
      "  0.3744896  0.284809   0.44235919]\n",
      " [0.         0.78980693 0.         0.         0.         0.\n",
      "  0.         0.         0.61335554]\n",
      " [0.         0.         0.4804584  0.63174505 0.         0.\n",
      "  0.         0.4804584  0.37311881]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The dog\",\n",
    "    \"The fox is quick\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into TF-IDF vectors\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print feature names (words)\n",
    "print(\"Feature names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f5bfd",
   "metadata": {},
   "source": [
    "# Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845563c7",
   "metadata": {},
   "source": [
    "Word2Vec is a popular technique in natural language processing (NLP) for learning distributed representations of words in a vector space. It maps words to continuous vector representations, capturing semantic similarities between words. There are two main types of Word2Vec models:\n",
    "\n",
    "Continuous Bag of Words (CBOW):\n",
    "\n",
    "    CBOW predicts the target word given its context (surrounding words within a fixed window).\n",
    "    \n",
    "    Use CBOW when you have a large dataset and you're more concerned about the speed of training. It's faster to train than \n",
    "    skipgram.\n",
    "    \n",
    "    Works well with frequent words.\n",
    "    \n",
    "Skip-gram:\n",
    "\n",
    "    Skip-gram predicts the context words given a target word.\n",
    "    Use Skip-gram when you have a small dataset or want to capture rare words and their relationships better.\n",
    "    Captures more information about infrequent words and phrases.\n",
    "    When to use each type depends on your specific task and data characteristics:\n",
    "\n",
    "If you have a large dataset and you're more concerned about training speed, CBOW might be a better choice.\n",
    "\n",
    "If you have a small dataset or you want to capture better relationships between rare words, Skip-gram might be more suitable.\n",
    "\n",
    "Skip-gram tends to perform better with large datasets and is more robust for infrequent words and phrases.\n",
    "\n",
    "CBOW is faster to train and often performs better for frequent words.\n",
    "\n",
    "In practice, it's often a good idea to try both CBOW and Skip-gram on your dataset and evaluate which one works better for your specific task. Additionally, pre-trained Word2Vec models are available for various languages and domains, which can be fine-tuned or used directly for downstream tasks such as sentiment analysis, named entity recognition, and machine translation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "168982ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081f29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "story = []\n",
    "for filename in os.listdir('data'):\n",
    "    \n",
    "    f = open(os.path.join('data',filename))\n",
    "    corpus = f.read()\n",
    "    raw_sent = sent_tokenize(corpus)\n",
    "    for sent in raw_sent:\n",
    "        story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accf379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec6e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b378235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6569794, 8628190)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(story, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349eddc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stormborn', 0.8133265376091003),\n",
       " ('targaryen', 0.7668876051902771),\n",
       " ('unburnt', 0.7458482980728149),\n",
       " ('myrcella', 0.736862301826477),\n",
       " ('princess', 0.7245938777923584),\n",
       " ('queen', 0.7145258188247681),\n",
       " ('rhaegar', 0.6532346606254578),\n",
       " ('elia', 0.64824378490448),\n",
       " ('margaery', 0.6471149325370789),\n",
       " ('dorne', 0.6423312425613403)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('daenerys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22784773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jon'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['jon','rikon','robb','arya','sansa','bran'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09bbf570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bronn'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['cersei', 'jaime', 'bronn', 'tyrion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d3b051c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8409555"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('arya','sansa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f39764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96538562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing the dimension using PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3cd9676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17453, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X = pca.fit_transform(model.wv.get_normed_vectors())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724fa67",
   "metadata": {},
   "source": [
    "Bag of Words (BoW):\n",
    "\n",
    "Use Case: Bag of Words is commonly used for text classification tasks where word order is not important, such as sentiment analysis or spam detection.\n",
    "\n",
    "Example: Suppose you're building a sentiment analysis system to classify movie reviews as positive or negative. You can represent each review as a bag of words, ignoring word order but capturing word frequencies.\n",
    "\n",
    "N-grams:\n",
    "\n",
    "Use Case: N-grams are useful when you want to capture local word order and context, such as in language modeling or machine translation.\n",
    "\n",
    "Example: In a language modeling task, you might use trigrams (3-grams) to predict the next word in a sentence based on the two preceding words. For instance, given the phrase \"the quick brown,\" you can predict the next word by considering trigrams like (\"the\", \"quick\", \"brown\").\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "\n",
    "Use Case: TF-IDF is effective for information retrieval tasks where you want to weigh the importance of words in documents based on their frequency and rarity across a corpus.\n",
    "\n",
    "Example: Consider a search engine where you want to retrieve relevant documents for a user query. TF-IDF helps identify documents containing rare words (which might be more informative) while penalizing common words.\n",
    "\n",
    "Word2Vec:\n",
    "\n",
    "Use Case: Word2Vec is ideal when you need to capture semantic relationships between words and represent them in a continuous vector space. It's often used for tasks like similarity detection, document clustering, or word analogy tasks.\n",
    "\n",
    "Example: In a document clustering task, you can use Word2Vec to represent each document as a vector and then cluster similar documents together based on the similarity of their vector representations.\n",
    "\n",
    "Each technique has its strengths and weaknesses, and the choice depends on the specific requirements of your task. For instance, if you need to capture word order and context, n-grams might be more suitable than bag of words. If you're interested in capturing semantic similarities between words, Word2Vec would be a better choice than TF-IDF. Understanding the characteristics and applicability of each technique will help you choose the most appropriate one for your NLP task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913362e9",
   "metadata": {},
   "source": [
    "GloVe, FastText, Doc2Vec, BERT, Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c09c93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
