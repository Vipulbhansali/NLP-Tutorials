{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "598ebd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3aa7afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load only the first 2000 rows of the CSV file\n",
    "data = pd.read_csv(r\"C:\\Users\\Vipul\\train.csv\", nrows=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29617299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af530efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bfdd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['id','severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e25c201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6bddcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'who', 'latter', 'since', 'put', 'often', 'much', 'call', 'hers', 'others', 'are', 'am', 'move', 'them', 'show', 'everywhere', 'two', 'may', 'alone', 'all', \"'m\", 'indeed', 'herself', '‘s', 'almost', 'quite', 'some', 'please', 'become', 'next', 'due', 'she', 'see', 'thereafter', 'back', 'himself', 'about', 'might', 'third', 'his', 'whereas', 'used', 'neither', 'under', 'at', 'while', 'mine', 'thereupon', 'amount', 'he', 'within', '’ll', 'empty', '‘m', 'yet', 'nevertheless', '‘ve', 'have', 'whatever', 'hence', 'many', 'should', 'beyond', 'eight', 'was', 'above', 'the', 'twelve', 'doing', 'were', 'you', 'namely', 'only', 'then', 'hereby', 'or', 'itself', 'of', 'again', 'where', '’m', 'nowhere', 'also', 'unless', 'not', '‘re', 'whereby', 'hundred', 'our', 'really', 'your', 'very', 'nobody', \"'re\", 'through', 'onto', 'somehow', 'off', 'besides', 'several', 'more', 'just', 'thus', 'otherwise', 'that', 'without', 'to', 'whither', 'sometimes', 'yours', 'afterwards', 'therein', 'part', 'whole', 'own', 'ours', 'below', 'will', 'name', 'front', 'us', 'out', 'my', 'seems', 'does', 'before', 'nor', 'has', 'elsewhere', 'four', 'rather', 'perhaps', 'fifteen', 'whom', 'how', 'noone', 'three', 'fifty', 'along', 'from', 'whether', 'seeming', 'ever', 'either', 'bottom', 'thru', \"'ll\", 'nothing', 'but', '’s', 'less', 'did', \"'d\", 'both', 'an', 'eleven', 'in', 'anywhere', 'sixty', 'herein', 'their', 'least', 'so', 'using', 'various', 'thereby', 'via', 'even', 'up', 'further', 'towards', 'forty', 'side', 'a', 'ca', 'yourselves', 'around', '‘d', 'same', 'however', 'i', 'except', 'me', 'someone', 'any', 'most', 'such', 'each', 'done', 'this', 'him', 'throughout', 'yourself', 'here', 'is', \"'s\", '’re', '’ve', 'ten', 'why', 'whenever', 'seemed', 'few', \"'ve\", 'sometime', 'there', 'these', 'mostly', 'per', 'everything', 'her', 'once', 'already', 'those', 'anyway', 'into', 'well', 'cannot', 'too', 'seem', 'amongst', 'former', 'which', 'beforehand', 'go', 'across', 'between', 'although', 'latterly', 'six', 'first', 'hereafter', 'becomes', 'do', 'whoever', 'moreover', 'make', 'on', 'everyone', 'than', 'no', 'myself', 'whereupon', 'therefore', 'if', 'they', 'with', 'always', 'give', 'because', 'n‘t', 'wherever', 'enough', 'other', 'ourselves', 'after', 'full', 'among', \"n't\", 'themselves', 'during', 'can', 'made', 'nine', 'it', 'became', 'thence', 'never', 'another', 'else', 'be', 'whence', 'last', 'becoming', 'until', 'serious', 'anything', 'hereupon', 'toward', 'we', '‘ll', 'been', 'being', 'regarding', 'say', 'for', 'none', 'one', 'now', 'five', 'still', 'though', 'wherein', 'as', 'twenty', 'when', 'against', 'and', 'something', 'n’t', 'meanwhile', 'by', 'take', 'keep', 'beside', 'get', 'whose', 'down', 'anyone', 'what', 'its', 'upon', 'top', 'whereafter', 'behind', 'could', 'formerly', 'must', 'anyhow', 'would', 'somewhere', 'over', 're', 'had', 'every', 'together', '’d'}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04928c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "punctuations = string.punctuation\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f3d73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    \n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "\n",
    "\n",
    "    # print(doc)\n",
    "    # print(type(doc))\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "\n",
    "    # print(mytokens)\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1379bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat', 'apple']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I am eating apple ?\"\n",
    "spacy_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb57faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(tokenizer = spacy_tokenizer)\n",
    "# tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64388529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vipul\\New_Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit_transform([\"I am eating apple, I like apple\",\"I am playing cricket\"]).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45058f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'cricket', 'eat', 'like', 'play'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8e18c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat': 2, 'apple': 0, 'like': 3, 'play': 4, 'cricket': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80deb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['comment_text'] # the features we want to analyze\n",
    "ylabels = data['toxic'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2,stratify=ylabels)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0facebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72660197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vipul\\New_Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_vetcors= count_vector.fit_transform(X_train)\n",
    "X_test_vetcors= count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1b40bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 10921)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vetcors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e224d98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 10921)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vetcors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81d70c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train_vetcors,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c28bfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.915\n",
      "Logistic Regression Precision: 0.8\n",
      "Logistic Regression Recall: 0.2\n"
     ]
    }
   ],
   "source": [
    "predicted = classifier.predict(X_test_vetcors)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ba1551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vipul\\New_Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "X_train_vetcors= tfidf_vector.fit_transform(X_train)\n",
    "X_test_vetcors= tfidf_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ef33c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.915\n",
      "Logistic Regression Precision: 1.0\n",
      "Logistic Regression Recall: 0.15\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_vetcors,y_train)\n",
    "predicted = classifier.predict(X_test_vetcors)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4e5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
